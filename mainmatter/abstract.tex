Datasets generated at different stages of cancer diagnosis and prognosis allow monitoring of 
disease progression or regression for specific patient treatment options.  Whether coming from magnetic resonance, 
DNA microarray, or epigenetic markers these datasets are all high dimensional and only relevant in their respective domains.  
As patient mortality decreases with early treatment of cancer, it is beneficial to have endogenous and exogenous insights into tumor the etiology at the initial stage of diagnosis.  Methods integrating and translating between different medical domains remains a challenging task and in our work we explain the challenges of exploring somatic mutation and medical imaging datasets.  We then develop a deep learning model that explains the process of inferring somatic mutations from a patient's corresponding medical image.

We first study the statistical and computational issues that arise when analyzing somatic mutation datasets.  Our goal is to identify the underlying biological structure that promotes tumor angiogenesis such that it can be used for patient differentiation and possible treatment targets.  Several popular solutions are presented, and we present some arguments in favor of
the method utilized and advocated in this thesis: Bayesian non-parametrics. This chosen
framework is the subject of a detailed review and we propose extensions of existing algorithms to efficiently find influential biological structure.

Similarly we analyze the local patterns found in medical images, often referred to as imaging features.  While the general idea of imaging features in medical images is not novel, we distinguish our method by generating features for modalities not often used, Positron Emission Tomography – Computed Tomography  and Diffusion Weighted Magnetic Resonance Imaging.   The proposed features are applied in machine learning algorithm for image classification to showcase their robustness.

The challenge of image-to-mutation translation is the different generative processes that create the initial datasets.  A medical image consists of discrete pixels on a 3D space, while mutations are a sparse count vector.  We follow the paradigm of creating abstract representations of both datasets in a shared latant space, but we propose a model that also includes domain specific abstractions in the latent space.  This allows our framework to preserve the structure of the original domains while incorporating information of another.  To train and test our model we create a build a unique pan cancer dataset of 1063 samples that compromises of point cloud representations of lesions and their corresponding somatic mutations.  

%%The central topic of our thesis is the correspondence between somatic mutations and medical image of lesions.  We build upon our previous work to help solve two issues in creating this model: the exact location of the lesion biopsy is unknown therefore we create a volumetric representation of the lesion, and the ability to account for the sparse and high dimensional nature of somatic mutation datasets.  To account for increasing modeling complexity we rely on the inference network of variational autoencoders (VAE).  Of course, as mentioned to full utilize VAEs, a larger dataset is needed, as such we build a pan cancer dataset of 1063 samples that compromises of point cloud representations of lesions and their corresponding somatic mutations.  This dataset is a specific subset of the already amazing work done in the collections of data from The Cancer Genomic Archive (TCGA) and The Cancer Imaging Archive (TCIA).  We demonstrate our model on this subset of data for predicting a patient's full somatic mutation profile on just the lesion images.   

%%Somatic mutations of cancerous lesions and their corresponding medical images offer endogenous and exogenous insights into tumor etiology for patient diagnosis and prognosis.  The confluence of mutational and imaging data has led to a new branch of informatics called imaging genomics which correlates the datasets for downstream tasks such as inference or prediction of mutations from a lesion image.  To account for the complexity and high dimensionality of both datasets, many models focus on a subset somatic mutations based on frequency, resulting in the common learning problem of overfitting, which is a cause of underutilizing the full dataset.  Therefore the models typically are only applicable to a small population of patients.  In this thesis, we exploit the pairing of Bayesian nonparametrics and inference networks to create a learning algorithm that translates imaging data into predictions of a patient's somatic mutation profile.

%%We start by considering the sparse phenomena of somatic mutation datasets, where there are many, few frequently occurring mutations and a long-tail of rare mutations.  Our goal is to decompose the dataset into interpretable features that are also biologically significant.  The standard algorithms of clustering or decomposition, such as Principal Component Analysis (PCA), of the data follows similar methodologies as seen in natural language process, but the same assumptions cannot be applied to biological systems.  A clustering approach assigns a mutation to a single cluster, however, a single mutation is known to belong in multiple distinct biological processes during tumor angiogenesis.  Analysis that decompose the data onto a smaller subspace assume the data is generated through a multinomial likelihood or by penalizing the likelihood using a penalty function such as elastic net for modeling sparsity.  A suitable method for analysis of somatic mutation datasets should explicitly model highly over-dispersed count data along with biological correlations of the mutations.  Through the use of beta-Bernoulli stochastic process (BeBP) one can examine an unbounded number of features that are not distinct in the set of mutations.  Our work shows robust learning of interpretable features of somatic mutation datasets with respect to the full mutational profile.  We demonstrate our work on the Pan Cancer Somatic Mutation Dataset, validating the interpretable features by linking them to biologically significance using over-representation analysis.  The underlying features can then be used for understanding the biology of the lesion. 


%%Before we explore the common subspace of somatic mutations and imaging, we aim to identify local patterns in lesion images for prediction of the histology of breast lesions and patient treatment effect.  Before, the rise of deep neural networks many images relied on hand-crafted features that exploited the texture and shape information available in pixels, however, these features cannot always be applied in medical imaging domain as there is low variance between pixels as in the case of Positron emission tomography–computed tomography (PET-CT) or they are not based on RGB values like in Diffusion Weighted Magnetic Resonance Imaging (DW-MRI).  Moreover, although a deep learning approach is ideal due to it's data-driven nature, the need for a large dataset limits it's use in some medical domains.  Our work studies imaging features from two different imaging modalities to determine the influence of imaging features on prediction scores.  For DW-MRI we study histogram features based on the parameter maps generated from two exponential functions, Continuous Time Random Walk (CTRW) and Intravoxel Incoherent Motion (IVIM), generated from high \textit{b}-values and low \textit{b}-values respectively.  We show that a combination of features from different parameter maps is superior to modeling the underlying lesion histology.  We then show the value of modeling a lesion in the 3D space as the prediction of treatment response is influenced by the intra-heterogenity of a lesion and as such a volumetric representation offers a comprehensive profile of the lesion.  Though 2D lesion images are the most common when performing prediction tasks for malignancy, 3D representation offer a richer image feature set to exploit for complex prediction tasks

 